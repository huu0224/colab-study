# -*- coding: utf-8 -*-
"""Balance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T3WlUXrxisGb5dduV87-GD8nY8Yw9zql
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

from google.colab import drive
drive.mount('/content/drive')

"""1) 데이터 준비"""

!head -n 5 /content/drive/MyDrive/Colab Notebooks/ML2024_A/balance.csv

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML2024_A/balance.csv', index_col=0)

df

X = df.drop('Balance', axis=1)
y = df['Balance']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

"""2) 상관계수 분석

방법1 : 연속형 특성의 상관계수와 레이브과의 상관계수 따로 구하기
"""

X_train.iloc[:, :6].corr()

np.corrcoef(X_train['Rating'], y_train)

np.corrcoef(X_train['Limit'], y_train)

"""방법2 : 연속형 특성과 레이블의 상관계수 한꺼번에 구하기"""

pd.concat([X_train.iloc[:, :6], y_train.to_frame()], axis=1).corr()

"""상관관계가 가장 강한 연속형 특성은 Limit와 Rating이며, 이 둘 중 Balance와 더 강한 상관관계를 갖는 특성은 Rating이므로 Limit 특성을 삭제!"""

X_train = X_train.drop('Limit', axis=1)
X_test = X_test.drop('Limit', axis=1)

"""3) 특성 전처리(인코딩+스케일링)

방법1 : 직접 코드로 처리
"""

X_train.columns

num = list(X_train.columns[:5])   #['Income', 'Rating', 'Cards', 'Age', 'Education']
cat = list(X_train.columns[5:])    #['Gender', 'Student', Married', 'Ethnicity']

X_train_num = X_train[num]
X_test_num = X_test[num]

X_train_cat = X_train[cat]
one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first', feature_name_combiner='concat')
one_hot_encoder.fit(X_train_cat)
X_train_cat_encoded = pd.DataFrame(one_hot_encoder.transform(X_train_cat),
                                   columns = one_hot_encoder.get_feature_names_out(),
                                   index=X_train_cat.index)
X_test_cat = X_test[cat]
X_test_cat_encoded = pd.DataFrame(one_hot_encoder.transform(X_test_cat),
                                   columns = one_hot_encoder.get_feature_names_out(),
                                   index=X_test_cat.index)

X_train_encoded = pd.concat([X_train_num, X_train_cat_encoded], axis=1)
X_test_encoded = pd.concat([X_test_num, X_test_cat_encoded], axis=1)

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train_encoded)
X_test_std = scaler.transform(X_test_encoded)

X_train_std

"""방법2 : ColumnTransformer 사용하기"""

from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer

num_pipeline = make_pipeline(StandardScaler())
cat_pipeline = make_pipeline(OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'), StandardScaler())
preprocessing = make_column_transformer(
        (num_pipeline, make_column_selector(dtype_include=np.number)),
        (cat_pipeline, make_column_selector(dtype_include=object))
    )
X_train_preprocessed = preprocessing.fit_transform(X_train)
X_test_preprocessed = preprocessing.transform(X_test)

X_train_preprocessed

"""4) 선형회귀"""

reg = LinearRegression()
reg.fit(X_train_std, y_train)
print('결정계수(R^2) =', reg.score( X_test_std, y_test))

reg.coef_

"""5) 회귀트리"""

depth_grid = {'max_depth': range(1, 30)}
tree_cv = GridSearchCV(estimator=DecisionTreeRegressor(random_state=1),
                    param_grid = depth_grid, cv = 10,
                    scoring='r2', refit = True, n_jobs=-1)
tree_cv.fit(X_train_std, y_train)
print(tree_cv.best_score_)
print(tree_cv.best_params_)
print(tree_cv.score(X_test_std, y_test))

"""6) 랜덤 포레스트"""

depth_grid = {'max_depth': range(1, 10)}
forest_cv = GridSearchCV(estimator=RandomForestRegressor(n_estimators=200, random_state=1, n_jobs=-1),
                    param_grid = depth_grid, cv = 10,
                    scoring='r2', refit = True, n_jobs=-1)
forest_cv.fit(X_train_std, y_train)
print(forest_cv.best_score_)
print(forest_cv.best_params_)
print(forest_cv.score(X_test_std, y_test))

"""7) 모형 비교

결정계수로 성능을 비교할 때, 선형회귀가 가장 뛰어나다. 뿐만 아니라, 계산량을 비교하면 선형회귀<회귀트리<랜덤포레스트 이므로 계산의 효율면에서도 선형회귀가 베스트 모형이다.
"""