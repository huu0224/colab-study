# -*- coding: utf-8 -*-
"""CovType.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQPS1UOPNH8YlQbFQr9zdSokF63ecIUE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import confusion_matrix, precision_score, recall_score

from google.colab import drive
drive.mount('/content/drive')

"""1) 데이터 준비"""

!head -n 5 /content/drive/MyDrive/Colab Notebooks/ML2024_A/forest.csv

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML2024_A/forest.csv')

df

X = df.drop('cov_type', axis=1)
y = df['cov_type']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

"""2) 로지스틱 회귀"""

pipe_logistic = make_pipeline(StandardScaler(), LogisticRegression(random_state=1, max_iter=300))
c_grid = {'logisticregression__C' : [0.01, 0.1, 1.0, 10, 100]}
logistic_cv = GridSearchCV(estimator=pipe_logistic,
                    param_grid = c_grid, cv = 10,
                    scoring='accuracy', refit = True, n_jobs=-1)
logistic_cv.fit(X_train, y_train)
print(logistic_cv.best_score_)
print(logistic_cv.best_params_)
print(logistic_cv.score(X_test, y_test))

"""3) k-NN 분류기"""

pipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())
k_grid = {'kneighborsclassifier__n_neighbors' : list(range(1, 10, 2))}
knn_cv = GridSearchCV(estimator=pipe_knn,
                    param_grid = k_grid, cv = 10,
                    scoring='accuracy', refit = True, n_jobs=-1)
knn_cv.fit(X_train, y_train)
print(knn_cv.best_score_)
print(knn_cv.best_params_)
print(knn_cv.score(X_test, y_test))

"""4) 결정트리 분류기"""

depth_grid = {'max_depth': range(1, 31)}
tree_cv = GridSearchCV(estimator=DecisionTreeClassifier(random_state=1),
                    param_grid = depth_grid, cv = 10,
                    scoring='accuracy', refit = True, n_jobs=-1)
tree_cv.fit(X_train, y_train)
print(tree_cv.best_score_)
print(tree_cv.best_params_)
print(tree_cv.score(X_test, y_test))

"""5) 랜덤 포레스트 분류기"""

depth_grid = {'max_depth': range(1, 11)}
forest_cv = GridSearchCV(estimator=RandomForestClassifier(n_estimators=200, random_state=1),
                    param_grid = depth_grid, cv = 10,
                    scoring='accuracy', refit = True, n_jobs=-1)
forest_cv.fit(X_train, y_train)
print(forest_cv.best_score_)
print(forest_cv.best_params_)
print(forest_cv.score(X_test, y_test))

"""6) AdaBoost 분류기"""

learningrate_grid = {'learning_rate': np.linspace(0.1, 1.0, 10)}
# Adjusted parameters for faster execution
ada_cv_fast = GridSearchCV(
    estimator=AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=50, random_state=1),
    param_grid = learningrate_grid, cv = 3, # Reduced cv to 3
    scoring='accuracy', refit = True)

ada_cv_fast.fit(X_train, y_train)

print("Adjusted AdaBoost Best CV Score:", ada_cv_fast.best_score_)
print("Adjusted AdaBoost Best Parameters:", ada_cv_fast.best_params_)
print("Adjusted AdaBoost Test Score:", ada_cv_fast.score(X_test, y_test))

"""7) 모형별 성능 비교"""

import pandas as pd

# 사용자로부터 제공받은 데이터
data = {
    '모형': ['로지스틱회귀', 'KNN', '트리', '랜덤포레스트', '아다부스트'],
    '최적 초모수': ['C=10', 'k=1', 'max_depth=15', 'max_depth=10', 'learning_rate=0.7'],
    '테스트 스코어': [0.668, 0.738, 0.732, 0.788, 0.840]
}

# DataFrame 생성
model_performance_custom = pd.DataFrame(data)

# '테스트 스코어' 열을 소수점 셋째 자리까지 포맷팅
model_performance_custom['테스트 스코어'] = model_performance_custom['테스트 스코어'].apply(lambda x: f'{x:.3f}')

# 인덱스를 숨기고, 헤더는 가운데 정렬, 모든 데이터 열은 왼쪽 정렬하는 스타일 적용 후 HTML 테이블로 표시
display(model_performance_custom.style.hide(axis="index").set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},
    {'selector': 'td:nth-child(1)', 'props': [('text-align', 'left')]},
    {'selector': 'td:nth-child(2)', 'props': [('text-align', 'left')]},
    {'selector': 'td:nth-child(3)', 'props': [('text-align', 'left')]} # 세 번째 데이터 열 (테스트 스코어)을 왼쪽 정렬
]))

"""8) 베스트 모형 성능 평가 지표"""

best_model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=9), n_estimators=200, learning_rate=0.7, algorithm='SAMME', random_state=1)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print('정밀도 =', precision_score(y_test, y_pred, average='weighted'))
print('재현율 =', recall_score(y_test, y_pred, average='weighted'))

"""방법1 : mlxtend의 heatmap 함수"""

from mlxtend.plotting import heatmap
conf_mat = confusion_matrix(y_test, y_pred)
hmap = heatmap(conf_mat, figsize=(8,8), row_names=list(range(1, 8)), column_names=list(range(1, 8)),cmap='Blues')

"""방법2 : seaborn의 heapmap 함수"""

import seaborn as sns
sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=list(range(1, 8)), yticklabels=list(range(1, 8)),cmap='Blues')